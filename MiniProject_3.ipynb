{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision.utils import save_image\n",
    "import torchvision"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T04:07:24.763772400Z",
     "start_time": "2023-08-04T04:07:23.322712900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T04:07:24.772850300Z",
     "start_time": "2023-08-04T04:07:24.766765100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T04:07:24.804057900Z",
     "start_time": "2023-08-04T04:07:24.772347900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fake_training_sample/fake_images-0005.png\n",
      "Saving fake_training_sample/fake_images-0010.png\n",
      "Saving fake_training_sample/fake_images-0015.png\n",
      "Saving fake_training_sample/fake_images-0020.png\n",
      "Saving fake_training_sample/fake_images-0025.png\n",
      "Saving fake_training_sample/fake_images-0030.png\n",
      "Saving fake_training_sample/fake_images-0035.png\n",
      "Saving fake_training_sample/fake_images-0040.png\n",
      "Saving fake_training_sample/fake_images-0045.png\n",
      "Saving fake_training_sample/fake_images-0050.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "latent_dim = 100\n",
    "num_epochs_gan = 50\n",
    "num_epochs_model = 10\n",
    "\n",
    "# Prepare the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "dataset = MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "# Function to train the GAN\n",
    "def train_gan():\n",
    "    generator = Generator(latent_dim, 28 * 28)\n",
    "    discriminator = Discriminator(28 * 28)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    generator_optimizer = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "    discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "    for epoch in range(num_epochs_gan):\n",
    "        for batch_idx, (real_images, _) in enumerate(dataloader):\n",
    "            real_images = real_images.view(-1, 28 * 28)\n",
    "\n",
    "            # Train the discriminator with real images\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            real_labels = torch.ones(real_images.size(0), 1)\n",
    "            real_pred = discriminator(real_images)\n",
    "            real_loss = criterion(real_pred, real_labels)\n",
    "            real_loss.backward()\n",
    "\n",
    "            # Train the discriminator with fake images (generated by the generator)\n",
    "            noise = torch.randn(real_images.size(0), latent_dim)\n",
    "            fake_images = generator(noise)\n",
    "            fake_labels = torch.zeros(real_images.size(0), 1)\n",
    "            fake_pred = discriminator(fake_images.detach())\n",
    "            fake_loss = criterion(fake_pred, fake_labels)\n",
    "            fake_loss.backward()\n",
    "\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "            # Train the generator to fool the discriminator\n",
    "            generator_optimizer.zero_grad()\n",
    "            noise = torch.randn(real_images.size(0), latent_dim)\n",
    "            fake_images = generator(noise)\n",
    "            fake_pred = discriminator(fake_images)\n",
    "            generator_loss = criterion(fake_pred, real_labels)\n",
    "            generator_loss.backward()\n",
    "\n",
    "            generator_optimizer.step()\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "            fake_fname = 'fake_training_sample/fake_images-{0:0=4d}.png'.format(epoch + 1)\n",
    "            print('Saving', fake_fname)\n",
    "            save_image(denorm(fake_images), fake_fname)\n",
    "    torch.save(generator.state_dict(), \"generator.pth\")\n",
    "    torch.save(discriminator.state_dict(), \"discriminator.pth\")\n",
    "# Train the GAN\n",
    "train_gan()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T04:18:06.594310400Z",
     "start_time": "2023-08-04T04:07:24.804057900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [100/938], Loss: 0.3914\n",
      "Epoch [1/15], Step [200/938], Loss: 0.3078\n",
      "Epoch [1/15], Step [300/938], Loss: 0.4380\n",
      "Epoch [1/15], Step [400/938], Loss: 0.1774\n",
      "Epoch [1/15], Step [500/938], Loss: 0.2936\n",
      "Epoch [1/15], Step [600/938], Loss: 0.4853\n",
      "Epoch [1/15], Step [700/938], Loss: 0.1584\n",
      "Epoch [1/15], Step [800/938], Loss: 0.2259\n",
      "Epoch [1/15], Step [900/938], Loss: 0.2769\n",
      "Epoch [2/15], Step [100/938], Loss: 0.1586\n",
      "Epoch [2/15], Step [200/938], Loss: 0.1905\n",
      "Epoch [2/15], Step [300/938], Loss: 0.3399\n",
      "Epoch [2/15], Step [400/938], Loss: 0.0947\n",
      "Epoch [2/15], Step [500/938], Loss: 0.1368\n",
      "Epoch [2/15], Step [600/938], Loss: 0.2923\n",
      "Epoch [2/15], Step [700/938], Loss: 0.5561\n",
      "Epoch [2/15], Step [800/938], Loss: 0.1021\n",
      "Epoch [2/15], Step [900/938], Loss: 0.3102\n",
      "Epoch [3/15], Step [100/938], Loss: 0.1538\n",
      "Epoch [3/15], Step [200/938], Loss: 0.0831\n",
      "Epoch [3/15], Step [300/938], Loss: 0.1020\n",
      "Epoch [3/15], Step [400/938], Loss: 0.0685\n",
      "Epoch [3/15], Step [500/938], Loss: 0.0958\n",
      "Epoch [3/15], Step [600/938], Loss: 0.0415\n",
      "Epoch [3/15], Step [700/938], Loss: 0.3027\n",
      "Epoch [3/15], Step [800/938], Loss: 0.2747\n",
      "Epoch [3/15], Step [900/938], Loss: 0.1171\n",
      "Epoch [4/15], Step [100/938], Loss: 0.0083\n",
      "Epoch [4/15], Step [200/938], Loss: 0.0608\n",
      "Epoch [4/15], Step [300/938], Loss: 0.1180\n",
      "Epoch [4/15], Step [400/938], Loss: 0.0680\n",
      "Epoch [4/15], Step [500/938], Loss: 0.0261\n",
      "Epoch [4/15], Step [600/938], Loss: 0.1404\n",
      "Epoch [4/15], Step [700/938], Loss: 0.1123\n",
      "Epoch [4/15], Step [800/938], Loss: 0.0313\n",
      "Epoch [4/15], Step [900/938], Loss: 0.1185\n",
      "Epoch [5/15], Step [100/938], Loss: 0.4453\n",
      "Epoch [5/15], Step [200/938], Loss: 0.0593\n",
      "Epoch [5/15], Step [300/938], Loss: 0.0940\n",
      "Epoch [5/15], Step [400/938], Loss: 0.0271\n",
      "Epoch [5/15], Step [500/938], Loss: 0.2653\n",
      "Epoch [5/15], Step [600/938], Loss: 0.1147\n",
      "Epoch [5/15], Step [700/938], Loss: 0.0562\n",
      "Epoch [5/15], Step [800/938], Loss: 0.0359\n",
      "Epoch [5/15], Step [900/938], Loss: 0.0241\n",
      "Epoch [6/15], Step [100/938], Loss: 0.0175\n",
      "Epoch [6/15], Step [200/938], Loss: 0.2169\n",
      "Epoch [6/15], Step [300/938], Loss: 0.0354\n",
      "Epoch [6/15], Step [400/938], Loss: 0.0798\n",
      "Epoch [6/15], Step [500/938], Loss: 0.1149\n",
      "Epoch [6/15], Step [600/938], Loss: 0.2083\n",
      "Epoch [6/15], Step [700/938], Loss: 0.0557\n",
      "Epoch [6/15], Step [800/938], Loss: 0.0629\n",
      "Epoch [6/15], Step [900/938], Loss: 0.0556\n",
      "Epoch [7/15], Step [100/938], Loss: 0.0088\n",
      "Epoch [7/15], Step [200/938], Loss: 0.0137\n",
      "Epoch [7/15], Step [300/938], Loss: 0.1059\n",
      "Epoch [7/15], Step [400/938], Loss: 0.2653\n",
      "Epoch [7/15], Step [500/938], Loss: 0.1282\n",
      "Epoch [7/15], Step [600/938], Loss: 0.0469\n",
      "Epoch [7/15], Step [700/938], Loss: 0.1695\n",
      "Epoch [7/15], Step [800/938], Loss: 0.0802\n",
      "Epoch [7/15], Step [900/938], Loss: 0.0803\n",
      "Epoch [8/15], Step [100/938], Loss: 0.0527\n",
      "Epoch [8/15], Step [200/938], Loss: 0.1039\n",
      "Epoch [8/15], Step [300/938], Loss: 0.0299\n",
      "Epoch [8/15], Step [400/938], Loss: 0.2914\n",
      "Epoch [8/15], Step [500/938], Loss: 0.0038\n",
      "Epoch [8/15], Step [600/938], Loss: 0.0409\n",
      "Epoch [8/15], Step [700/938], Loss: 0.1838\n",
      "Epoch [8/15], Step [800/938], Loss: 0.0458\n",
      "Epoch [8/15], Step [900/938], Loss: 0.2194\n",
      "Epoch [9/15], Step [100/938], Loss: 0.0841\n",
      "Epoch [9/15], Step [200/938], Loss: 0.1616\n",
      "Epoch [9/15], Step [300/938], Loss: 0.1616\n",
      "Epoch [9/15], Step [400/938], Loss: 0.1149\n",
      "Epoch [9/15], Step [500/938], Loss: 0.0206\n",
      "Epoch [9/15], Step [600/938], Loss: 0.0840\n",
      "Epoch [9/15], Step [700/938], Loss: 0.0348\n",
      "Epoch [9/15], Step [800/938], Loss: 0.0346\n",
      "Epoch [9/15], Step [900/938], Loss: 0.0388\n",
      "Epoch [10/15], Step [100/938], Loss: 0.1681\n",
      "Epoch [10/15], Step [200/938], Loss: 0.1473\n",
      "Epoch [10/15], Step [300/938], Loss: 0.0090\n",
      "Epoch [10/15], Step [400/938], Loss: 0.0264\n",
      "Epoch [10/15], Step [500/938], Loss: 0.1186\n",
      "Epoch [10/15], Step [600/938], Loss: 0.0654\n",
      "Epoch [10/15], Step [700/938], Loss: 0.0665\n",
      "Epoch [10/15], Step [800/938], Loss: 0.0236\n",
      "Epoch [10/15], Step [900/938], Loss: 0.0662\n",
      "Epoch [11/15], Step [100/938], Loss: 0.0735\n",
      "Epoch [11/15], Step [200/938], Loss: 0.0256\n",
      "Epoch [11/15], Step [300/938], Loss: 0.1718\n",
      "Epoch [11/15], Step [400/938], Loss: 0.0553\n",
      "Epoch [11/15], Step [500/938], Loss: 0.0080\n",
      "Epoch [11/15], Step [600/938], Loss: 0.0940\n",
      "Epoch [11/15], Step [700/938], Loss: 0.0344\n",
      "Epoch [11/15], Step [800/938], Loss: 0.0700\n",
      "Epoch [11/15], Step [900/938], Loss: 0.2370\n",
      "Epoch [12/15], Step [100/938], Loss: 0.0268\n",
      "Epoch [12/15], Step [200/938], Loss: 0.0994\n",
      "Epoch [12/15], Step [300/938], Loss: 0.1574\n",
      "Epoch [12/15], Step [400/938], Loss: 0.0340\n",
      "Epoch [12/15], Step [500/938], Loss: 0.1297\n",
      "Epoch [12/15], Step [600/938], Loss: 0.0238\n",
      "Epoch [12/15], Step [700/938], Loss: 0.0226\n",
      "Epoch [12/15], Step [800/938], Loss: 0.0212\n",
      "Epoch [12/15], Step [900/938], Loss: 0.0081\n",
      "Epoch [13/15], Step [100/938], Loss: 0.0849\n",
      "Epoch [13/15], Step [200/938], Loss: 0.1617\n",
      "Epoch [13/15], Step [300/938], Loss: 0.0433\n",
      "Epoch [13/15], Step [400/938], Loss: 0.0377\n",
      "Epoch [13/15], Step [500/938], Loss: 0.1031\n",
      "Epoch [13/15], Step [600/938], Loss: 0.0494\n",
      "Epoch [13/15], Step [700/938], Loss: 0.0904\n",
      "Epoch [13/15], Step [800/938], Loss: 0.0609\n",
      "Epoch [13/15], Step [900/938], Loss: 0.1172\n",
      "Epoch [14/15], Step [100/938], Loss: 0.0646\n",
      "Epoch [14/15], Step [200/938], Loss: 0.2061\n",
      "Epoch [14/15], Step [300/938], Loss: 0.0916\n",
      "Epoch [14/15], Step [400/938], Loss: 0.0164\n",
      "Epoch [14/15], Step [500/938], Loss: 0.1798\n",
      "Epoch [14/15], Step [600/938], Loss: 0.0349\n",
      "Epoch [14/15], Step [700/938], Loss: 0.0231\n",
      "Epoch [14/15], Step [800/938], Loss: 0.0109\n",
      "Epoch [14/15], Step [900/938], Loss: 0.0661\n",
      "Epoch [15/15], Step [100/938], Loss: 0.1331\n",
      "Epoch [15/15], Step [200/938], Loss: 0.0467\n",
      "Epoch [15/15], Step [300/938], Loss: 0.0349\n",
      "Epoch [15/15], Step [400/938], Loss: 0.0323\n",
      "Epoch [15/15], Step [500/938], Loss: 0.1846\n",
      "Epoch [15/15], Step [600/938], Loss: 0.0445\n",
      "Epoch [15/15], Step [700/938], Loss: 0.0075\n",
      "Epoch [15/15], Step [800/938], Loss: 0.2144\n",
      "Epoch [15/15], Step [900/938], Loss: 0.0682\n",
      "Training finished!\n",
      "Accuracy on the test set: 96.36%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.003\n",
    "num_epochs = 15\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model\n",
    "class DigitRecognizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitRecognizer, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input image\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = DigitRecognizer()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T04:20:48.063891200Z",
     "start_time": "2023-08-04T04:18:06.599311Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1500, 1, 28, 28])\n",
      "torch.Size([1500])\n"
     ]
    }
   ],
   "source": [
    "def generate_synthetic_data(num_samples):\n",
    "    generator = Generator(latent_dim, 28 * 28)\n",
    "    generator.load_state_dict(torch.load(\"generator.pth\"))\n",
    "    generator.eval()\n",
    "\n",
    "    synthetic_data = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            noise = torch.randn(1, latent_dim)\n",
    "            generated_image = generator(noise).view(-1, 1, 28, 28)\n",
    "            synthetic_data.append(generated_image)\n",
    "\n",
    "    synthetic_data = torch.cat(synthetic_data, dim=0)\n",
    "    return synthetic_data\n",
    "\n",
    "# Function to label synthetic data using the trained discriminator\n",
    "def label_synthetic_data(synthetic_data):\n",
    "    labels_ = list()\n",
    "    with torch.no_grad():\n",
    "        scores = model(synthetic_data)\n",
    "        for tensor in scores:\n",
    "            labels_.append(torch.argmax(tensor).item())\n",
    "    #print(labels_)\n",
    "    return labels_\n",
    "\n",
    "# Combine real and synthetic data to create an augmented dataset\n",
    "def augment_data(num_synthetic_samples):\n",
    "    # real_data = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    # real_targets = dataset.targets\n",
    "\n",
    "    synthetic_data = generate_synthetic_data(num_synthetic_samples)\n",
    "\n",
    "    #synthetic_data = synthetic_data.view(-1, 28*28)  # Flatten the synthetic data to match discriminator's input size\n",
    "\n",
    "    synthetic_targets = torch.full((num_synthetic_samples,), 10, dtype=torch.int64)  # Use label 10 for synthetic data\n",
    "\n",
    "    # Create the TensorDataset for real and synthetic data\n",
    "    # real_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    # synthetic_dataset = torch.utils.data.TensorDataset(synthetic_data, synthetic_targets)\n",
    "\n",
    "    # Combine the real and synthetic datasets using ConcatDataset\n",
    "    #augmented_dataset = ConcatDataset([real_dataset, synthetic_dataset])\n",
    "\n",
    "    # Create DataLoader for the augmented dataset\n",
    "    #augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    # Label the synthetic data using the trained discriminator\n",
    "    synthetic_labels = label_synthetic_data(synthetic_data)\n",
    "    synthetic_labels = torch.tensor(synthetic_labels, dtype=torch.long)\n",
    "    #synthetic_labels = (synthetic_labels - 0.5) / 0.5\n",
    "    #synthetic_labels = synthetic_labels.to(torch.long)\n",
    "\n",
    "    print(synthetic_data.view(-1, 1, 28, 28).size())\n",
    "    print(synthetic_labels.size())\n",
    "    # Update the synthetic targets with the predicted labels\n",
    "    synthetic_dataset = torch.utils.data.TensorDataset(synthetic_data.view(-1, 1, 28, 28), synthetic_labels)\n",
    "\n",
    "    # Combine the real and labeled synthetic datasets using ConcatDataset\n",
    "    #augmented_dataset = ConcatDataset([real_dataset, synthetic_dataset])\n",
    "\n",
    "    # Create DataLoader for the augmented dataset\n",
    "    #transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    augmented_dataloader = DataLoader(synthetic_dataset, batch_size=batch_size, shuffle=True)\n",
    "    #print(synthetic_labels[1])\n",
    "    #print(synthetic_labels)\n",
    "    return augmented_dataloader\n",
    "\n",
    "num_synthetic_samples = 1500\n",
    "#augmented_data, augmented_targets = augment_data(num_synthetic_samples)\n",
    "aug = augment_data(num_synthetic_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T04:20:48.353193500Z",
     "start_time": "2023-08-04T04:20:48.063891200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [100/24], Loss: 0.5064\n",
      "Epoch [1/15], Step [200/24], Loss: 0.2799\n",
      "Epoch [1/15], Step [300/24], Loss: 0.4024\n",
      "Epoch [1/15], Step [400/24], Loss: 0.3244\n",
      "Epoch [1/15], Step [500/24], Loss: 0.2423\n",
      "Epoch [1/15], Step [600/24], Loss: 0.2242\n",
      "Epoch [1/15], Step [700/24], Loss: 0.2922\n",
      "Epoch [1/15], Step [800/24], Loss: 0.2387\n",
      "Epoch [1/15], Step [900/24], Loss: 0.1081\n",
      "Epoch [2/15], Step [100/24], Loss: 0.2970\n",
      "Epoch [2/15], Step [200/24], Loss: 0.1944\n",
      "Epoch [2/15], Step [300/24], Loss: 0.2548\n",
      "Epoch [2/15], Step [400/24], Loss: 0.1838\n",
      "Epoch [2/15], Step [500/24], Loss: 0.1592\n",
      "Epoch [2/15], Step [600/24], Loss: 0.2140\n",
      "Epoch [2/15], Step [700/24], Loss: 0.1050\n",
      "Epoch [2/15], Step [800/24], Loss: 0.1623\n",
      "Epoch [2/15], Step [900/24], Loss: 0.1229\n",
      "Epoch [3/15], Step [100/24], Loss: 0.1020\n",
      "Epoch [3/15], Step [200/24], Loss: 0.1625\n",
      "Epoch [3/15], Step [300/24], Loss: 0.1015\n",
      "Epoch [3/15], Step [400/24], Loss: 0.0810\n",
      "Epoch [3/15], Step [500/24], Loss: 0.1150\n",
      "Epoch [3/15], Step [600/24], Loss: 0.1749\n",
      "Epoch [3/15], Step [700/24], Loss: 0.2206\n",
      "Epoch [3/15], Step [800/24], Loss: 0.1318\n",
      "Epoch [3/15], Step [900/24], Loss: 0.3199\n",
      "Epoch [4/15], Step [100/24], Loss: 0.1311\n",
      "Epoch [4/15], Step [200/24], Loss: 0.0890\n",
      "Epoch [4/15], Step [300/24], Loss: 0.0975\n",
      "Epoch [4/15], Step [400/24], Loss: 0.1973\n",
      "Epoch [4/15], Step [500/24], Loss: 0.0881\n",
      "Epoch [4/15], Step [600/24], Loss: 0.0971\n",
      "Epoch [4/15], Step [700/24], Loss: 0.1346\n",
      "Epoch [4/15], Step [800/24], Loss: 0.0905\n",
      "Epoch [4/15], Step [900/24], Loss: 0.0523\n",
      "Epoch [5/15], Step [100/24], Loss: 0.0529\n",
      "Epoch [5/15], Step [200/24], Loss: 0.1953\n",
      "Epoch [5/15], Step [300/24], Loss: 0.0360\n",
      "Epoch [5/15], Step [400/24], Loss: 0.2566\n",
      "Epoch [5/15], Step [500/24], Loss: 0.1209\n",
      "Epoch [5/15], Step [600/24], Loss: 0.0326\n",
      "Epoch [5/15], Step [700/24], Loss: 0.1079\n",
      "Epoch [5/15], Step [800/24], Loss: 0.0915\n",
      "Epoch [5/15], Step [900/24], Loss: 0.2225\n",
      "Epoch [6/15], Step [100/24], Loss: 0.1942\n",
      "Epoch [6/15], Step [200/24], Loss: 0.1042\n",
      "Epoch [6/15], Step [300/24], Loss: 0.2852\n",
      "Epoch [6/15], Step [400/24], Loss: 0.0848\n",
      "Epoch [6/15], Step [500/24], Loss: 0.0422\n",
      "Epoch [6/15], Step [600/24], Loss: 0.1416\n",
      "Epoch [6/15], Step [700/24], Loss: 0.0810\n",
      "Epoch [6/15], Step [800/24], Loss: 0.0638\n",
      "Epoch [6/15], Step [900/24], Loss: 0.1627\n",
      "Epoch [7/15], Step [100/24], Loss: 0.1248\n",
      "Epoch [7/15], Step [200/24], Loss: 0.1086\n",
      "Epoch [7/15], Step [300/24], Loss: 0.2260\n",
      "Epoch [7/15], Step [400/24], Loss: 0.0997\n",
      "Epoch [7/15], Step [500/24], Loss: 0.2684\n",
      "Epoch [7/15], Step [600/24], Loss: 0.0883\n",
      "Epoch [7/15], Step [700/24], Loss: 0.0188\n",
      "Epoch [7/15], Step [800/24], Loss: 0.0317\n",
      "Epoch [7/15], Step [900/24], Loss: 0.1967\n",
      "Epoch [8/15], Step [100/24], Loss: 0.0779\n",
      "Epoch [8/15], Step [200/24], Loss: 0.0513\n",
      "Epoch [8/15], Step [300/24], Loss: 0.1521\n",
      "Epoch [8/15], Step [400/24], Loss: 0.0992\n",
      "Epoch [8/15], Step [500/24], Loss: 0.3127\n",
      "Epoch [8/15], Step [600/24], Loss: 0.1934\n",
      "Epoch [8/15], Step [700/24], Loss: 0.2128\n",
      "Epoch [8/15], Step [800/24], Loss: 0.4171\n",
      "Epoch [8/15], Step [900/24], Loss: 0.0771\n",
      "Epoch [9/15], Step [100/24], Loss: 0.4665\n",
      "Epoch [9/15], Step [200/24], Loss: 0.0539\n",
      "Epoch [9/15], Step [300/24], Loss: 0.1375\n",
      "Epoch [9/15], Step [400/24], Loss: 0.1126\n",
      "Epoch [9/15], Step [500/24], Loss: 0.0543\n",
      "Epoch [9/15], Step [600/24], Loss: 0.0706\n",
      "Epoch [9/15], Step [700/24], Loss: 0.0564\n",
      "Epoch [9/15], Step [800/24], Loss: 0.1095\n",
      "Epoch [9/15], Step [900/24], Loss: 0.1199\n",
      "Epoch [10/15], Step [100/24], Loss: 0.1900\n",
      "Epoch [10/15], Step [200/24], Loss: 0.0554\n",
      "Epoch [10/15], Step [300/24], Loss: 0.0993\n",
      "Epoch [10/15], Step [400/24], Loss: 0.0020\n",
      "Epoch [10/15], Step [500/24], Loss: 0.0632\n",
      "Epoch [10/15], Step [600/24], Loss: 0.0770\n",
      "Epoch [10/15], Step [700/24], Loss: 0.0463\n",
      "Epoch [10/15], Step [800/24], Loss: 0.1512\n",
      "Epoch [10/15], Step [900/24], Loss: 0.0524\n",
      "Epoch [11/15], Step [100/24], Loss: 0.0122\n",
      "Epoch [11/15], Step [200/24], Loss: 0.1599\n",
      "Epoch [11/15], Step [300/24], Loss: 0.0708\n",
      "Epoch [11/15], Step [400/24], Loss: 0.1439\n",
      "Epoch [11/15], Step [500/24], Loss: 0.0294\n",
      "Epoch [11/15], Step [600/24], Loss: 0.0720\n",
      "Epoch [11/15], Step [700/24], Loss: 0.0402\n",
      "Epoch [11/15], Step [800/24], Loss: 0.0531\n",
      "Epoch [11/15], Step [900/24], Loss: 0.1150\n",
      "Epoch [12/15], Step [100/24], Loss: 0.1314\n",
      "Epoch [12/15], Step [200/24], Loss: 0.0199\n",
      "Epoch [12/15], Step [300/24], Loss: 0.0375\n",
      "Epoch [12/15], Step [400/24], Loss: 0.0137\n",
      "Epoch [12/15], Step [500/24], Loss: 0.1063\n",
      "Epoch [12/15], Step [600/24], Loss: 0.0688\n",
      "Epoch [12/15], Step [700/24], Loss: 0.0549\n",
      "Epoch [12/15], Step [800/24], Loss: 0.0540\n",
      "Epoch [12/15], Step [900/24], Loss: 0.0512\n",
      "Epoch [13/15], Step [100/24], Loss: 0.0206\n",
      "Epoch [13/15], Step [200/24], Loss: 0.0101\n",
      "Epoch [13/15], Step [300/24], Loss: 0.1704\n",
      "Epoch [13/15], Step [400/24], Loss: 0.0643\n",
      "Epoch [13/15], Step [500/24], Loss: 0.2246\n",
      "Epoch [13/15], Step [600/24], Loss: 0.0179\n",
      "Epoch [13/15], Step [700/24], Loss: 0.0405\n",
      "Epoch [13/15], Step [800/24], Loss: 0.1084\n",
      "Epoch [13/15], Step [900/24], Loss: 0.0807\n",
      "Epoch [14/15], Step [100/24], Loss: 0.1068\n",
      "Epoch [14/15], Step [200/24], Loss: 0.0668\n",
      "Epoch [14/15], Step [300/24], Loss: 0.0096\n",
      "Epoch [14/15], Step [400/24], Loss: 0.1164\n",
      "Epoch [14/15], Step [500/24], Loss: 0.0164\n",
      "Epoch [14/15], Step [600/24], Loss: 0.0804\n",
      "Epoch [14/15], Step [700/24], Loss: 0.0566\n",
      "Epoch [14/15], Step [800/24], Loss: 0.0822\n",
      "Epoch [14/15], Step [900/24], Loss: 0.0061\n",
      "Epoch [15/15], Step [100/24], Loss: 0.1614\n",
      "Epoch [15/15], Step [200/24], Loss: 0.0955\n",
      "Epoch [15/15], Step [300/24], Loss: 0.0423\n",
      "Epoch [15/15], Step [400/24], Loss: 0.0287\n",
      "Epoch [15/15], Step [500/24], Loss: 0.1088\n",
      "Epoch [15/15], Step [600/24], Loss: 0.0688\n",
      "Epoch [15/15], Step [700/24], Loss: 0.0268\n",
      "Epoch [15/15], Step [800/24], Loss: 0.0188\n",
      "Epoch [15/15], Step [900/24], Loss: 0.0273\n",
      "Accuracy on the test set: 84.93%\n"
     ]
    }
   ],
   "source": [
    "model2 = DigitRecognizer()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(aug)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(aug):\n",
    "            # Convert augmented data to float tensors if needed\n",
    "        images = images.float()  # Change to .to(torch.float) if you have integer tensors\n",
    "\n",
    "    # Convert model's parameters to the same data type as the augmented data\n",
    "        for param in model2.parameters():\n",
    "            param.data = param.data.to(images.dtype)\n",
    "\n",
    "        outputs = model2(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "            # Convert augmented data to float tensors if needed\n",
    "        images = images.float()  # Change to .to(torch.float) if you have integer tensors\n",
    "\n",
    "    # Convert model's parameters to the same data type as the augmented data\n",
    "        for param in model2.parameters():\n",
    "            param.data = param.data.to(images.dtype)\n",
    "\n",
    "        outputs = model2(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# Test the model\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in aug:\n",
    "        outputs = model2(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T04:23:37.064024300Z",
     "start_time": "2023-08-04T04:20:48.356194200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(aug))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T04:23:37.067505400Z",
     "start_time": "2023-08-04T04:23:37.065023700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
